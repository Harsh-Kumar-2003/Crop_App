{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "612796fd-22c4-4cf4-b10b-9ea31dcc167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import models, layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e46c27b-9ad8-456d-95cc-ce392b15360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 32\n",
    "CHANNELS = 3\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9807c652-ce74-47fa-8bdd-5f951e272bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "517385c9-e51a-4bee-8e6b-b6127b5326f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'Apple___Apple_scab': 800 train images, 200 test images\n",
      "Folder 'Apple___Black_rot': 800 train images, 200 test images\n",
      "Folder 'Apple___Cedar_apple_rust': 800 train images, 200 test images\n",
      "Folder 'Apple___healthy': 1316 train images, 329 test images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Base directory containing your subfolders\n",
    "base_dir = r\"D:\\Crop_App\\apple train\"\n",
    "\n",
    "# Directories to store the train and test splits\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Get subfolders (skip 'train' and 'test' if they already exist)\n",
    "subfolders = [folder for folder in os.listdir(base_dir)\n",
    "              if os.path.isdir(os.path.join(base_dir, folder)) and folder not in [\"train\", \"test\"]]\n",
    "\n",
    "# Loop over each subfolder to split images\n",
    "for folder in subfolders:\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    # List image files (adjust extensions as needed)\n",
    "    images = [f for f in os.listdir(folder_path)\n",
    "              if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif'))]\n",
    "    \n",
    "    # Split images into train (80%) and test (20%) sets\n",
    "    train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create subdirectories for this class in train and test folders\n",
    "    train_class_dir = os.path.join(train_dir, folder)\n",
    "    test_class_dir = os.path.join(test_dir, folder)\n",
    "    os.makedirs(train_class_dir, exist_ok=True)\n",
    "    os.makedirs(test_class_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy training images\n",
    "    for image in train_images:\n",
    "        src = os.path.join(folder_path, image)\n",
    "        dst = os.path.join(train_class_dir, image)\n",
    "        shutil.copy(src, dst)\n",
    "    \n",
    "    # Copy testing images\n",
    "    for image in test_images:\n",
    "        src = os.path.join(folder_path, image)\n",
    "        dst = os.path.join(test_class_dir, image)\n",
    "        shutil.copy(src, dst)\n",
    "    \n",
    "    print(f\"Folder '{folder}': {len(train_images)} train images, {len(test_images)} test images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e992079f-0d64-46a4-bb81-8587c2f30a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3716 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path= r\"D:\\Crop_App\\apple train\\train\"\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_path, \n",
    "    shuffle=True,\n",
    "    image_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42da61f2-7527-4ef7-8701-92ac7e6eaf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 929 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_path= r\"D:\\Crop_App\\apple train\\test\"\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_path, \n",
    "    shuffle=False,\n",
    "    image_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c9a212-6673-4e55-82cb-300271a477dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))\n",
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6282e292-44f4-41bf-ac81-64b47bdb3c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().shuffle(1000).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecc53395-dec9-486e-adbf-f3893af5023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1991e7f7-c017-4d08-b32d-b4d2c4319340",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4  # Number of bird species\n",
    "\n",
    "def preprocess(image, label):\n",
    "    label = tf.one_hot(label, depth=num_classes)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(preprocess)\n",
    "test_ds = test_ds.map(preprocess)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e095cbe6-0b5d-4aca-a1b0-c76eb27a49ae",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Replace Flatten with GlobalAveragePooling2D\n",
    "    GlobalAveragePooling2D(),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70f2d1a7-1606-4a2b-bfea-3128ff6caa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "722668f5-909a-449e-b664-c141c81dd2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define your data generator with the desired augmentations\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Rescale the images to [0, 1]\n",
    "    rotation_range=15,  # Randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # Randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # Randomly shift images vertically (fraction of total height)\n",
    "    shear_range=0.1,  # Randomly shear images\n",
    "    zoom_range=0.1,  # Randomly zoom images\n",
    "    horizontal_flip=True,  # Randomly flip images horizontally\n",
    "    fill_mode='nearest'  # Fill in any pixels lost during augmentation\n",
    ")\n",
    "\n",
    "# Define your validation data generator (usually without augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5f93a27-7a42-449d-b827-9558751d4891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3716 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,  # This is the source directory for training images\n",
    "    target_size=(224, 224),  # All images will be resized to 224x224\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552e00e-eb45-4558-a086-2fae530991b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 929 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "359b3f31-5769-456a-9e11-4efffe289058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint directory and file name\n",
    "checkpoint_path = 'D:/Crop_App/cp-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,  # Save only the best model based on validation loss\n",
    "    save_freq='epoch'     # Save after every epoch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fde90ac3-9419-4074-b3f5-6b08e0180d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09ef530f-5477-4a19-8303-88d94a8d511c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "116/116 [==============================] - 37s 246ms/step - loss: 1.1342 - accuracy: 0.4902 - val_loss: 0.7725 - val_accuracy: 0.7015\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.77248, saving model to D:/Crop_App\\cp-0001.ckpt\n",
      "Epoch 2/25\n",
      "116/116 [==============================] - 32s 272ms/step - loss: 0.8377 - accuracy: 0.6691 - val_loss: 0.6922 - val_accuracy: 0.7414\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.77248 to 0.69216, saving model to D:/Crop_App\\cp-0002.ckpt\n",
      "Epoch 3/25\n",
      "116/116 [==============================] - 33s 279ms/step - loss: 0.6272 - accuracy: 0.7495 - val_loss: 0.4296 - val_accuracy: 0.8567\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69216 to 0.42961, saving model to D:/Crop_App\\cp-0003.ckpt\n",
      "Epoch 4/25\n",
      "116/116 [==============================] - 27s 235ms/step - loss: 0.5278 - accuracy: 0.7994 - val_loss: 0.4579 - val_accuracy: 0.8287\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.42961\n",
      "Epoch 5/25\n",
      "116/116 [==============================] - 28s 242ms/step - loss: 0.4587 - accuracy: 0.8214 - val_loss: 0.3526 - val_accuracy: 0.8685\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.42961 to 0.35258, saving model to D:/Crop_App\\cp-0005.ckpt\n",
      "Epoch 6/25\n",
      "116/116 [==============================] - 32s 280ms/step - loss: 0.4303 - accuracy: 0.8374 - val_loss: 0.4243 - val_accuracy: 0.8416\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35258\n",
      "Epoch 7/25\n",
      "116/116 [==============================] - 32s 273ms/step - loss: 0.3937 - accuracy: 0.8575 - val_loss: 0.7189 - val_accuracy: 0.7446\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35258\n",
      "Epoch 8/25\n",
      "116/116 [==============================] - 30s 255ms/step - loss: 0.4375 - accuracy: 0.8366 - val_loss: 0.3635 - val_accuracy: 0.8556\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35258\n",
      "Epoch 9/25\n",
      "116/116 [==============================] - 27s 235ms/step - loss: 0.3173 - accuracy: 0.8825 - val_loss: 0.3596 - val_accuracy: 0.8567\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35258\n",
      "Epoch 10/25\n",
      "116/116 [==============================] - 32s 273ms/step - loss: 0.2950 - accuracy: 0.8898 - val_loss: 0.3547 - val_accuracy: 0.8631\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35258\n",
      "Epoch 11/25\n",
      "116/116 [==============================] - 30s 253ms/step - loss: 0.2906 - accuracy: 0.8944 - val_loss: 0.2519 - val_accuracy: 0.9009\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.35258 to 0.25195, saving model to D:/Crop_App\\cp-0011.ckpt\n",
      "Epoch 12/25\n",
      "116/116 [==============================] - 28s 238ms/step - loss: 0.2968 - accuracy: 0.8963 - val_loss: 0.2487 - val_accuracy: 0.9019\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.25195 to 0.24872, saving model to D:/Crop_App\\cp-0012.ckpt\n",
      "Epoch 13/25\n",
      "116/116 [==============================] - 27s 236ms/step - loss: 0.2255 - accuracy: 0.9202 - val_loss: 0.4554 - val_accuracy: 0.8675\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.24872\n",
      "Epoch 14/25\n",
      "116/116 [==============================] - 27s 236ms/step - loss: 0.2330 - accuracy: 0.9175 - val_loss: 0.3274 - val_accuracy: 0.8815\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.24872\n",
      "Epoch 15/25\n",
      "116/116 [==============================] - 27s 236ms/step - loss: 0.2020 - accuracy: 0.9340 - val_loss: 0.2897 - val_accuracy: 0.9041\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.24872\n",
      "Epoch 16/25\n",
      "116/116 [==============================] - 30s 259ms/step - loss: 0.1911 - accuracy: 0.9340 - val_loss: 0.2285 - val_accuracy: 0.9213\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.24872 to 0.22847, saving model to D:/Crop_App\\cp-0016.ckpt\n",
      "Epoch 17/25\n",
      "116/116 [==============================] - 28s 239ms/step - loss: 0.1836 - accuracy: 0.9319 - val_loss: 0.3357 - val_accuracy: 0.8890\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.22847\n",
      "Epoch 18/25\n",
      "116/116 [==============================] - 35s 306ms/step - loss: 0.1804 - accuracy: 0.9378 - val_loss: 0.2070 - val_accuracy: 0.9278\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.22847 to 0.20699, saving model to D:/Crop_App\\cp-0018.ckpt\n",
      "Epoch 19/25\n",
      "116/116 [==============================] - 29s 252ms/step - loss: 0.1798 - accuracy: 0.9365 - val_loss: 0.2497 - val_accuracy: 0.9041\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.20699\n",
      "Epoch 20/25\n",
      "116/116 [==============================] - 30s 261ms/step - loss: 0.1536 - accuracy: 0.9463 - val_loss: 0.4147 - val_accuracy: 0.8879\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.20699\n",
      "Epoch 21/25\n",
      "116/116 [==============================] - 28s 244ms/step - loss: 0.1759 - accuracy: 0.9354 - val_loss: 0.1851 - val_accuracy: 0.9429\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.20699 to 0.18510, saving model to D:/Crop_App\\cp-0021.ckpt\n",
      "Epoch 22/25\n",
      "116/116 [==============================] - 28s 240ms/step - loss: 0.1926 - accuracy: 0.9324 - val_loss: 0.2926 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.18510\n",
      "Epoch 23/25\n",
      "116/116 [==============================] - 28s 242ms/step - loss: 0.1557 - accuracy: 0.9468 - val_loss: 0.1174 - val_accuracy: 0.9601\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.18510 to 0.11737, saving model to D:/Crop_App\\cp-0023.ckpt\n",
      "Epoch 24/25\n",
      "116/116 [==============================] - 28s 240ms/step - loss: 0.1621 - accuracy: 0.9370 - val_loss: 0.1439 - val_accuracy: 0.9483\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.11737\n",
      "Epoch 25/25\n",
      "116/116 [==============================] - 28s 237ms/step - loss: 0.1552 - accuracy: 0.9452 - val_loss: 0.1612 - val_accuracy: 0.9418\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.11737\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples // test_generator.batch_size,\n",
    "    verbose=1,\n",
    "    callbacks=[cp_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c6c3b9e-00d2-4cc8-95d9-914d7a9b958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: apple_disease_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('apple_disease_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b1bc5a-a35e-4ee0-b0ac-b838dd57234d",
   "metadata": {},
   "source": [
    "**Prediction Part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c98f435-7906-487b-af4a-29d1290589d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import image module \n",
    "from IPython.display import Image \n",
    "  \n",
    "# get the image \n",
    "img = Image(url=\"apple train/test/Apple___Apple_scab/image (107).JPG\", width=224, height=224) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abd9107f-0662-4293-ba68-be15d5182119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"apple train/test/Apple___Apple_scab/image (107).JPG\" width=\"256\" height=\"256\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b2af02c-0281-4e06-a6e1-7f4599a2ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# Load the image\n",
    "img = Image.open(\"apple train/Apple___Black_rot/image (100).JPG\")\n",
    "\n",
    "# Resize the image to the size your model expects\n",
    "img = img.resize((224, 224))\n",
    "\n",
    "# Convert the image to a NumPy array\n",
    "img_array = np.array(img)\n",
    "\n",
    "# Scale pixel values (if your model was trained with scaled images)\n",
    "img_array = img_array / 255.0\n",
    "\n",
    "# Expand dimensions so the image has a batch dimension\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Now you can pass this to model.predict\n",
    "prediction = model.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06c06c21-6a4e-4e4c-80ae-834978732e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.3241153e-04, 9.9956721e-01, 3.3634076e-13, 3.0020945e-07]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba5341f4-99ae-465e-8bea-52ef0e3f3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = np.argmax(prediction)\n",
    "predicted_class_name = CLASS_NAMES[predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2543008f-a494-470d-a402-4ba7927edfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apple___Black_rot'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d7189-ad0c-4c9f-9afe-e329dd2de7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
